\chapter{An Introduction to Parallel Computing}
\pss{week13-crop}{1}{0.96}{}{\cht{parallel computing}}
\psd{week13-crop}{2}{3}{0.96}{Avoid parallel computing at all costs}{..}{}
\psd{week13-crop}{4}{5}{0.96}{..}{Coarse grain parallelism}{\cht{coarse grain parallelism}}
\psd{week13-crop}{6}{7}{0.96}{Medium grain parallelism}{Fine grain parallelism}{\cht{fine grain parallelism}\cht{medium grain parallelism}}
\psd{week13-crop}{8}{9}{0.96}{Shared memory architectures}{Distributed memory architectures}{\cht{distributed memory}\cht{shared memory}}
\psd{week13-crop}{10}{11}{0.96}{Types of architectures}{Parallel machines}{\cht{clusters}\cht{parallel machines}\cht{SISD}\cht{SIMD}\cht{SPMD}\cht{MIMD}}
\psd{week13-crop}{12}{13}{0.96}{Distributed memory}{Network topologies}{\cht{network topologies}}
\psd{week13-crop}{14}{15}{0.96}{Message passing on distributed memory architectures}{What is a message?}{\cht{message}}
\psd{week13-crop}{16}{17}{0.96}{Sending and receiving a message}{The structure of an MPI program}{\cht{MPI}}
\psd{week13-crop}{18}{19}{0.96}{Optaining rank and size}{Sending and receiving a message}{}
\psd{week13-crop}{20}{21}{0.96}{MPI_Send, MPI_Recv}{Sending and receiving a message}{}
\psd{week13-crop}{22}{23}{0.96}{Using MPI on D-PHYS machines}{Things to do on your own machine}{}
\psd{week13-crop}{24}{25}{0.96}{Probing for messages}{Deadlocks}{\cht{deadlocks}}
\psd{week13-crop}{26}{27}{0.96}{Blocking communication types}{Nonblocking communication types}{\cht{blocking communication}\cht{non-blocking communication}}
\psd{week13-crop}{28}{29}{0.96}{Collective communication}{Types of collective communication}{\cht{broadcast}}
\psd{week13-crop}{30}{31}{0.96}{Scatter & Gather}{One-way communication}{\cht{scatter}\cht{gather}}
\psd{week13-crop}{32}{33}{0.96}{SPMD style}{Master-slave style}{\cht{master-slave communication}\cht{SPMD}}
\psd{week13-crop}{34}{35}{0.96}{Amdahl's law}{Shared memory}{\cht{Amdahl's law}}
\psd{week13-crop}{36}{37}{0.96}{Shared memory architectures}{OpenMP}{\cht{OpenMP}\cht{shared memory architectures}}
\psd{week13-crop}{38}{39}{0.96}{OpenMP: A first parallel example}{Parallel region}{\cht{parallel region}}
\psd{week13-crop}{40}{41}{0.96}{Parallel sum with OpenMP}{..}{}
\psd{week13-crop}{42}{43}{0.96}{Running a program with OpenMP}{..}{}
\psd{week13-crop}{44}{45}{0.96}{..}{Perfect scaling}{}
\psd{week13-crop}{46}{47}{0.96}{Race condition}{Dot product with OpenMP}{\cht{dot product}\cht{race condition}}
\psd{week13-crop}{48}{49}{0.96}{..}{Penna model with OpenMP}{\cht{Penna model}}
\psd{week13-crop}{50}{51}{0.96}{..}{OpenMP tasks}{\cht{tasks}}
\psd{week13-crop}{52}{53}{0.96}{OpenMP conclusions}{OpenMP is more than this}{}
